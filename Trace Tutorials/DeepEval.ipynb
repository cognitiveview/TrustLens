{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be548dd0",
   "metadata": {},
   "source": [
    "### DeepEval configuration Guide\n",
    "\n",
    "This notebook demonstrates the basic usage of the `deepeval` library. We'll cover:\n",
    "\n",
    "- Logging test cases  \n",
    "- Running evaluations  \n",
    "- Viewing and saving results locally  \n",
    "- Evaluating DeepEval metrics through the Trace metrics API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a762d624",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install deepeval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3c6ca9",
   "metadata": {},
   "source": [
    "## RAG Test Case: \n",
    "\n",
    "In this example, we define a **Retrieval-Augmented Generation (RAG)** test case using `deepeval`. The goal is to evaluate how well a language model's response aligns with both the expected output and the retrieved context.\n",
    "\n",
    "### What We're Doing\n",
    "\n",
    "- **Input**: A user asks _\"What causes seasonal color changes in leaves?\"_\n",
    "- **Actual Output**: The model's generated response.\n",
    "- **Expected Output**: A reference answer used for comparison.\n",
    "- **Context**: The full context provided to the model for generation.\n",
    "- **Retrieval Context**: The subset of documents retrieved for grounding the answer.\n",
    "\n",
    "We use `LLMTestCase` to encapsulate this information, which will later be evaluated using various DeepEval metrics such as:\n",
    "- `AnswerRelevancyMetric`\n",
    "- `ContextualRelevancyMetric`\n",
    "- `ContextualRecallMetric`\n",
    "- `ContextualPrecisionMetric`\n",
    "- `FaithfulnessMetric`\n",
    "- `HallucinationMetric`\n",
    "\n",
    "This setup allows us to assess factual consistency, grounding, and hallucination risk in RAG-based systems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3582124",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.metrics import (\n",
    "    AnswerRelevancyMetric,\n",
    "    ContextualRelevancyMetric,\n",
    "    ContextualRecallMetric,\n",
    "    ContextualPrecisionMetric,\n",
    "    FaithfulnessMetric,\n",
    "    HallucinationMetric\n",
    ")\n",
    "\n",
    "# Define RAG test case with context and retrieval_context\n",
    "tc = LLMTestCase(\n",
    "    input=\"What causes seasonal color changes in leaves?\",\n",
    "    actual_output=\"Leaves change color due to reduced chlorophyll production in fall, revealing carotenoids and anthocyanins. Temperature and light changes trigger this process.\",\n",
    "    expected_output=\"Seasonal leaf color changes are primarily caused by the breakdown of chlorophyll in autumn, revealing underlying pigments like carotenoids (yellows/oranges) and anthocyanins (reds/purples), triggered by shorter days and cooler temperatures.\",\n",
    "    context=[\n",
    "        \"Photosynthesis slows in autumn due to reduced sunlight and temperature changes.\",\n",
    "        \"Chlorophyll breaks down faster than it's produced, unmasking existing carotenoids.\",\n",
    "        \"Anthocyanins are newly synthesized in some species as sugars become trapped in leaves.\",\n",
    "        \"The process is influenced by both photoperiod (day length) and temperature changes.\"\n",
    "    ],\n",
    "    retrieval_context=[\n",
    "        \"Photosynthesis slows in autumn due to reduced sunlight and temperature changes.\",\n",
    "        \"Chlorophyll breaks down faster than it's produced, unmasking existing carotenoids.\",\n",
    "        \"Anthocyanins are newly synthesized in some species as sugars become trapped in leaves.\",\n",
    "        \"The process is influenced by both photoperiod (day length) and temperature changes.\"\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472625d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "OPENAI_API_KEY=\"Your OpenAI API Key Here\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c26cb4",
   "metadata": {},
   "source": [
    "## Metric Evaluation\n",
    "\n",
    "In this step, we initialize a set of evaluation metrics with custom thresholds and apply them to our RAG test case.\n",
    "\n",
    "### Metrics Used\n",
    "\n",
    "- `AnswerRelevancyMetric (≥ 0.7)`: Measures how relevant the model's answer is to the input question.\n",
    "- `ContextualRelevancyMetric (≥ 0.8)`: Evaluates how well the answer relates to the provided context.\n",
    "- `ContextualRecallMetric (≥ 0.9)`: Measures how much relevant information from the context is included in the output.\n",
    "- `ContextualPrecisionMetric (≥ 0.85)`: Measures how much of the output is grounded in the relevant context.\n",
    "- `FaithfulnessMetric (≥ 0.9)`: Checks whether the generated answer is faithful to the source context.\n",
    "- `HallucinationMetric (≤ 0.1)`: Detects content in the answer that is not supported by the context.\n",
    "\n",
    "### Evaluation Loop\n",
    "\n",
    "Each metric is applied to the test case using the `measure()` method. The results are stored in a list as dictionaries containing:\n",
    "- `metric_key`: The name of the metric\n",
    "- `value`: The computed score\n",
    "\n",
    "These results can be used for reporting, logging, or visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034e5682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize metrics with appropriate thresholds\n",
    "metrics = [\n",
    "    AnswerRelevancyMetric(threshold=0.7),\n",
    "    ContextualRelevancyMetric(threshold=0.8),\n",
    "    ContextualRecallMetric(threshold=0.9),\n",
    "    ContextualPrecisionMetric(threshold=0.85),\n",
    "    FaithfulnessMetric(threshold=0.9),\n",
    "    HallucinationMetric(threshold=0.1)\n",
    "]\n",
    "\n",
    "# Evaluate all metrics\n",
    "metric_results = {}\n",
    "for m in metrics:\n",
    "    m.measure(tc)\n",
    "    metric_results[m.__class__.__name__] = m.score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4c73fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AnswerRelevancyMetric': 1.0, 'ContextualRelevancyMetric': 1.0, 'ContextualRecallMetric': 1.0, 'ContextualPrecisionMetric': 1.0, 'FaithfulnessMetric': 1.0, 'HallucinationMetric': 0.0}\n"
     ]
    }
   ],
   "source": [
    "print(metric_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea0640f",
   "metadata": {},
   "source": [
    "## Posting Evaluation Metrics to CognitiveView Trace API\n",
    "\n",
    "This script sends the evaluation metric results (e.g., from DeepEval) to the CognitiveView Trace API using an authenticated HTTP POST request.\n",
    "\n",
    "### Authentication\n",
    "\n",
    "- Uses an **Authorization token** (`AUTH_TOKEN`) for secure access to the API.\n",
    "- Includes an **X-User-Id** header to identify the user performing the operation.\n",
    "\n",
    "### Endpoint\n",
    "\n",
    "- **Base URL**: `https://api.cognitiveview.com`\n",
    "- **API Path**: `/cv/v1/metrics`\n",
    "- **Full Endpoint**: `https://api.cognitiveview.com/cv/v1/metrics`\n",
    "\n",
    "### Payload Structure\n",
    "\n",
    "#### `metric_metadata`\n",
    "Metadata describing the context of the evaluation:\n",
    "- `application_name`: Name of the application being evaluated.\n",
    "- `version`: Version of the application.\n",
    "- `resource_name`: The evaluated resource (e.g., a model or endpoint).\n",
    "- `resource_id`: Unique ID of the resource.\n",
    "- `url`: The endpoint URL of the resource.\n",
    "- `provider`: Source of the metric system (e.g., `deepeval`).\n",
    "- `use_case`: The business or functional use case (e.g., `transportation`).\n",
    "\n",
    "#### `metric_data`\n",
    "Data containing the metric scores:\n",
    "- `resource_id`: The ID of the instance or model run being scored.\n",
    "- `resource_name`: Name of the evaluated resource.\n",
    "- `deepeval`: Dictionary of computed metric scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801caffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "BASE_URL = \"https://api.cognitiveview.com\"\n",
    "AUTH_TOKEN =\"Your-Authorization-Token-Here\"  # Replace with your actual token\n",
    "url = f\"{BASE_URL}/cv/v1/metrics\"\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": AUTH_TOKEN,\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"X-User-Id\": \"C473421_T181751\",  # Replace with your actual user ID\n",
    "}\n",
    "\n",
    "payload = {\n",
    "  \"metric_metadata\": {\n",
    "    \"application_name\": \"chat-application\",\n",
    "    \"version\": \"1.0.0\",\n",
    "    \"resource_name\": \"chat-completion\",\n",
    "    \"resource_id\": \"R-756\",\n",
    "    \"url\": \"https://api.example.com/chat\",\n",
    "    \"provider\": \"deepeval\",\n",
    "    \"use_case\": \"transportation\"\n",
    "  },\n",
    "  \"metric_data\": {\n",
    "    \"resource_id\": \"res_123456\",\n",
    "    \"resource_name\": \"chat-completion\",\n",
    "    \"deepeval\": metric_results\n",
    "  } \n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=payload)\n",
    "\n",
    "# Output the response\n",
    "print(f\"Status Code: {response.status_code}\")\n",
    "print(\"Response JSON:\", response.json())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
